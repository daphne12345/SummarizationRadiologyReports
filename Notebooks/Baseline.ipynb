{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQgZ9usXStcp"
   },
   "source": [
    "# Baseline model: Encoder-Decoder\n",
    "Authors: Elisa Nguyen and Daphne Theodorakopoulos\n",
    "\n",
    "Modified version based on Tutorial from https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAsrQdOiS2vy"
   },
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42241,
     "status": "ok",
     "timestamp": 1592811764777,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "IBaF9NxhhxX6",
    "outputId": "6db7c0c9-bc72-4443-97d8-c5737a509e47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46063,
     "status": "ok",
     "timestamp": 1592811768615,
     "user": {
      "displayName": "Thi Que-Lam Elisa Nguyen",
      "photoUrl": "",
      "userId": "14516834551380015257"
     },
     "user_tz": -120
    },
    "id": "uhC9-Bj0hzd8",
    "outputId": "eb22c115-7489-4667-be89-ef8f25ec678d"
   },
   "outputs": [],
   "source": [
    "import rouge\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_colwidth\", 200)          \n",
    "from tensorflow.keras.layers import Layer, Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, LSTM\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.engine import base_layer_v1\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops, math_ops\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tensorflow.python.ops import array_ops\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vip3cJBKS7rZ"
   },
   "source": [
    "## 2. Import data\n",
    "\n",
    "- load test, train, val\n",
    "- tokenize and convert findings and conclusions to numerical sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcdPPZ6Sh1Lv"
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_excel('x_train.xlsx')\n",
    "y_train = pd.read_excel('y_train.xlsx')\n",
    "x_val = pd.read_excel('x_val.xlsx')\n",
    "y_val = pd.read_excel('y_val.xlsx')\n",
    "x_test = pd.read_excel('x_test.xlsx')\n",
    "y_test = pd.read_excel('y_test.xlsx')\n",
    "\n",
    "max_len_text = 100 #max324 median 46\n",
    "max_len_summary = 32 #max219 median12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PQCPXX5h2u5"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "x_tokenizer.fit_on_texts(list(x_train['cleaned_text']))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_train['text_sequence'] = x_tokenizer.texts_to_sequences(x_train['cleaned_text'])\n",
    "x_val['text_sequence'] = x_tokenizer.texts_to_sequences(x_val['cleaned_text'])\n",
    "x_test['text_sequence'] = x_tokenizer.texts_to_sequences(x_test['cleaned_text'])\n",
    "\n",
    "#padding zero up to maximum length\n",
    "x_train['text_sequence'] = x_train['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_text, padding='post')[0]) \n",
    "x_val['text_sequence'] = x_val['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_text, padding='post')[0])\n",
    "x_test['text_sequence'] = x_test['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_text, padding='post')[0])\n",
    "\n",
    "x_voc_size = len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wweIMV-sh4Kr"
   },
   "outputs": [],
   "source": [
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer(filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "y_tokenizer.fit_on_texts(list(y_train['cleaned_summary']))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_train['text_sequence'] = y_tokenizer.texts_to_sequences(y_train['cleaned_summary']) \n",
    "y_val['text_sequence']= y_tokenizer.texts_to_sequences(y_val['cleaned_summary']) \n",
    "y_test['text_sequence']= y_tokenizer.texts_to_sequences(y_test['cleaned_summary']) \n",
    "\n",
    "#padding zero up to maximum length\n",
    "y_train['text_sequence'] = y_train['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_summary, padding='post')[0])\n",
    "y_val['text_sequence']= y_val['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_summary, padding='post')[0])\n",
    "y_test['text_sequence'] = y_test['text_sequence'].apply(lambda x: pad_sequences([x], maxlen= max_len_summary, padding='post')[0])\n",
    "\n",
    "y_voc_size = len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPq-WwGZTZab"
   },
   "source": [
    "## 3. Set up model functions\n",
    "\n",
    "- Model definition\n",
    "- Model inference\n",
    "- Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AnVEjAMJh53v"
   },
   "outputs": [],
   "source": [
    "def define_model(latent_dim):\n",
    "  \"\"\"\n",
    "  Function to define the Encoder-Decoder model\n",
    "  :param latent_dim: int, number of hidden units per layer\n",
    "  :returns model with decoder layers, encoder input & output & states, decoder input\n",
    "  \"\"\"\n",
    "  with tf.device('/device:GPU:0'):\n",
    "      K.clear_session() \n",
    "      latent_dim = latent_dim \n",
    "\n",
    "      # Encoder \n",
    "      encoder_inputs = Input(shape=(max_len_text,)) \n",
    "      enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "      #LSTM 1 \n",
    "      encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "      encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "      #LSTM 2 \n",
    "      encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "      encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "      #LSTM 3 \n",
    "      encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "      encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "      # Set up the decoder. \n",
    "      decoder_inputs = Input(shape=(None,)) \n",
    "      dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "      dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "      #LSTM using encoder_states as initial state\n",
    "      decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "      decoder_outputs1,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "      #Dense layer\n",
    "      decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "      decoder_outputs = decoder_dense(decoder_outputs1) \n",
    "\n",
    "      # Define the model\n",
    "      model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "      model.summary()\n",
    "      return model, encoder_inputs, encoder_outputs, state_h, state_c, decoder_inputs, dec_emb_layer, decoder_lstm, decoder_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irgV0fbF6HFM"
   },
   "outputs": [],
   "source": [
    "def model_inference(latent_dim, encoder_inputs, encoder_outputs, state_h, state_c, decoder_inputs, dec_emb_layer, decoder_lstm, decoder_dense): \n",
    "    \"\"\"\n",
    "    Function to do model inference to freeze the trained model and use it\n",
    "    :param latent_dim: int, number of hidden units in each layer\n",
    "    :param encoder_inputs: Input object for encoder\n",
    "    :param encoder_outputs: tensor, representing context vector\n",
    "    :param state_h: tensor, hidden state of the last encoder unit\n",
    "    :param state_c: tensor, cell state of the last encoder unit\n",
    "    :param decoder_inputs: Input object for decoder\n",
    "    :param dec_emb_layer: Embedding layer of the decoder\n",
    "    :param decoder_lstm: LSTM layer of the decoder\n",
    "    :param decoder_dense: Dense layer of the decoder\n",
    "    :returns inferred encoder model, inferred decoder model\n",
    "    \"\"\"\n",
    "    with tf.device('/device:GPU:0'):\n",
    "      # encoder inference\n",
    "      encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "      # decoder inference\n",
    "      # Below tensors will hold the states of the previous time step\n",
    "      decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "      decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "      decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "      # Get the embeddings of the decoder sequence\n",
    "      dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "      # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "      decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "      # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "      decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "      # Final decoder model\n",
    "      decoder_model = Model(\n",
    "      [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + [state_h2, state_c2])\n",
    "      return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDiCAcuK6atu"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "  \"\"\"\n",
    "  Function to decode an input sequence, so to generate a conclusion to a finding\n",
    "  :param input_seq: sequence of ints, numerical representation of preprocessed findings\n",
    "  :param encoder_model: inferred encoder model\n",
    "  :param decoder_model: inferred decoder model\n",
    "  :returns decoded sequence as text (string)\n",
    "  \"\"\"\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    reverse_target_word_index=y_tokenizer.index_word \n",
    "    reverse_source_word_index=x_tokenizer.index_word \n",
    "    target_word_index=y_tokenizer.word_index\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i=0\n",
    "    while not stop_condition:\n",
    "    #decoder model predicted irgendwo einen output token der nur ne prediction von 0 hat\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_token='end'\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "        i += 1\n",
    "        if i == (max_len_text +1):\n",
    "            stop_condition=True\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGTPEXCf6gcy"
   },
   "outputs": [],
   "source": [
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           limit_length=False,\n",
    "                           alpha=0.5, \n",
    "                           stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-HBNwny6iNu"
   },
   "outputs": [],
   "source": [
    "def birads_score(row):\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    pre = ['birads classificatie', 'birads', 'birads-classificatie', 'birads calcificatie']\n",
    "    sec = [' rechts', ' links', ' beiderzijds', ' beide','',' ',':',' code','-code', ' categorie', ' echografisch']\n",
    "    mid = ['','-',' ','- ', ' echografisch ', ' rechts ', ' links ', ' beiderzijds ', ' thans ']\n",
    "    dic = {0:['0'],1:['1','i', 'een'], 2:['l2','2','ii', 'twee'], 3:['3','iii', 'drie'], \n",
    "         4:['4','iv', 'vier'], 5:['5', 'vijf','v'], 6:['6','vi', 'zes']}\n",
    "    for i in range(6,-1,-1): \n",
    "        for p in pre: \n",
    "            for s in sec:\n",
    "                for m in mid:\n",
    "                    for val in dic[i]:\n",
    "                        if (p +s+ m+ val) in row:\n",
    "                            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMtLl8AZWgVL"
   },
   "source": [
    "## 3. Training and testing\n",
    "\n",
    "- train model on training set, with early stopping measured on loss on validation set\n",
    "- use trained model to generate conclusions for test set\n",
    "- evaluate using ROUGE and accuracy\n",
    "- write all results to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keyeRLZy6j6E"
   },
   "outputs": [],
   "source": [
    "latent_dim = 120\n",
    "batch_size = 128\n",
    "df_result = pd.DataFrame(columns=['findings', 'original', 'score', 'predicted', 'birads_predicted', 'rouge', 'birads_accuracy', 'epoch', 'val_loss'])\n",
    "df_result['findings']= x_test['cleaned_text']\n",
    "df_result['original'] = y_test['cleaned_summary']\n",
    "df_result['score'] = x_test['score']\n",
    "x_test_reshaped = [x_test['text_sequence'][i].reshape(1,max_len_text) for i in range(len(x_test))]\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "      #define model\n",
    "      model, encoder_inputs, encoder_outputs, state_h, state_c, decoder_inputs, dec_emb_layer, decoder_lstm, decoder_dense = define_model(latent_dim)\n",
    "\n",
    "      #compile and train model\n",
    "      model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "      es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True, patience=20)\n",
    "      history=model.fit([np.array(list(x_train['text_sequence'])), np.array(list(y_train['text_sequence']))[:,:-1]], np.array(list(y_train['text_sequence'])).reshape(np.array(list(y_train['text_sequence'])).shape[0],np.array(list(y_train['text_sequence'])).shape[1], 1)[:,1:] ,epochs=100,callbacks=[es],batch_size=batch_size, validation_data=([np.array(list(x_test['text_sequence'])),np.array(list(y_test['text_sequence']))[:,:-1]], np.array(list(y_test['text_sequence'])).reshape(np.array(list(y_test['text_sequence'])).shape[0],np.array(list(y_test['text_sequence'])).shape[1], 1)[:,1:]))\n",
    "      model.save_weights('/content/drive/My Drive/REDI/Models/Hyperparam_tuning/baseline_model')\n",
    "\n",
    "      #decode\n",
    "      encoder_model, decoder_model = model_inference(latent_dim, encoder_inputs, encoder_outputs, state_h, state_c, decoder_inputs, dec_emb_layer, decoder_lstm, decoder_dense)\n",
    "      encoder_model.save('/content/drive/My Drive/REDI/Models/Hyperparam_tuning/result/baseline_encoder_dim' + str(latent_dim) + 'bs' + str(batch_size))\n",
    "      decoder_model.save('/content/drive/My Drive/REDI/Models/Hyperparam_tuning/result/baseline_decoder_dim' + str(latent_dim) + 'bs' + str(batch_size))\n",
    "      df_result['predicted'] = pd.Series(x_test_reshaped).apply(lambda x: decode_sequence([x], encoder_model=encoder_model, decoder_model=decoder_model))\n",
    "      df_result['birads_predicted'] = df_result['predicted'].apply(birads_score)\n",
    "\n",
    "      #evaluate\n",
    "      all_hypothesis = list(df_result['predicted'])\n",
    "      all_references = list(df_result['original'])\n",
    "      scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "      df_result.loc[0, 'rouge'] = [scores]\n",
    "      df_result.loc[0, 'birads_accuracy'] = df_result[df_result['score']==df_result['birads_predicted']].shape[0]/df_result.shape[0]\n",
    "      df_result.loc[0, 'epoch'] = np.array(history.history['val_loss']).argmin()\n",
    "      df_result.loc[0, 'val_loss'] = history.history['val_loss'][np.array(history.history['val_loss']).argmin()]\n",
    "              \n",
    "      #save results\n",
    "      df_result.to_excel('baseline_model.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x67ad4BAiNSS"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKphkWVCi_A2"
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "      df_result['predicted'] = pd.Series(x_test_reshaped).apply(lambda x: decode_sequence([x], encoder_model=encoder_model, decoder_model=decoder_model))\n",
    "      df_result['birads_predicted'] = df_result['predicted'].apply(birads_score)\n",
    "\n",
    "      #evaluate\n",
    "      all_hypothesis = list(df_result['predicted'])\n",
    "      all_references = list(df_result['original'])\n",
    "      scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "      df_result.loc[0, 'rouge'] = [scores]\n",
    "      df_result.loc[0, 'birads_accuracy'] = df_result[df_result['score']==df_result['birads_predicted']].shape[0]/df_result.shape[0]\n",
    "      df_result.loc[0, 'epoch'] = np.array(history.history['val_loss']).argmin()\n",
    "      df_result.loc[0, 'val_loss'] = history.history['val_loss'][np.array(history.history['val_loss']).argmin()]\n",
    "              \n",
    "      #save results\n",
    "      df_result.to_excel('baseline_model.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
